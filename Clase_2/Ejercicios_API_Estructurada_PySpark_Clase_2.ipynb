{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DORclqBQDT3y",
        "outputId": "4d31b117-8637-4440-a5e2-8d65865cffec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descarguemos la data con la que trabajaremos\n",
        "Ejecuta la celda nada mas"
      ],
      "metadata": {
        "id": "PrjoH0fVDkUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import requests\n",
        "\n",
        "with open(\"./credit_cards_data.zip\", \"wb\") as f:\n",
        "    f.write(requests.get(\"https://github.com/engcarlosperezmolero/resources_and_tools/blob/main/data/csv/credit_cards_data.zip?raw=true\").content)\n",
        "\n",
        "zip_ref = ZipFile(f\"/content/credit_cards_data.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "lg3b4tXcDkY9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### application_record.csv\n",
        "|Columna| Explicacion|\n",
        "|-------|------------|\n",
        "|ID |\tClient number\t|\n",
        "|CODE_GENDER |\tGender\t|\n",
        "|FLAG_OWN_CAR |\tIs there a car\t|\n",
        "|FLAG_OWN_REALTY |\tIs there a property\t|\n",
        "|CNT_CHILDREN |\tNumber of children\t|\n",
        "|AMT_INCOME_TOTAL |\tAnnual income\t|\n",
        "|NAME_INCOME_TYPE |\tIncome category\t|\n",
        "|NAME_EDUCATION_TYPE |\tEducation level\t|\n",
        "|NAME_FAMILY_STATUS |\tMarital status\t|\n",
        "|NAME_HOUSING_TYPE |\tWay of living\t|\n",
        "|DAYS_BIRTH |\tBirthday\tCount backwards from current day (0), -1 means yesterday|\n",
        "|DAYS_EMPLOYED |\tStart date of employment\tCount backwards from current day(0). If positive, it means the person currently unemployed. |\n",
        "|FLAG_MOBIL |\tIs there a mobile phone\t|\n",
        "|FLAG_WORK_PHONE |\tIs there a work phone\t|\n",
        "|FLAG_PHONE |\tIs there a phone\t|\n",
        "|FLAG_EMAIL |\tIs there an email\t|\n",
        "|OCCUPATION_TYPE |\tOccupation\t|\n",
        "|CNT_FAM_MEMBERS |\tFamily size\t|\n",
        "\n"
      ],
      "metadata": {
        "id": "3OwdiK4g0wSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### credit_record.csv\n",
        "|Columna|Explicacion|\n",
        "|-------|------------|\n",
        "|ID     |\tClient number\t|\n",
        "|MONTHS_BALANCE|Record month The month of the extracted data is the starting point, backwards, 0 is the current month, -1 is the previous month, and so on|\n",
        "|STATUS|\tStatus\t0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month|\n",
        "|TOY_STATUS|3 estados de juguete que nos permitiran practicar. Los valores son: good, bad, regular|"
      ],
      "metadata": {
        "id": "bPsZlYnx3f5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rutas a los archivos\n",
        "```'/content/credit_cards_data/application_record.csv'```\n",
        "\n",
        "```'/content/credit_cards_data/credit_records_toy.csv'```\n"
      ],
      "metadata": {
        "id": "ASHhukpI18yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cree la sesion de Spark donde la aplicacion se llame como usted desee."
      ],
      "metadata": {
        "id": "utU2uwfrCWT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "XJ2bSXNUChA9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Ejercicio Api Estructurada Humai\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "iyq5bhExd-pK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lee los archivos dejando que spark infiera los tipos de las columnas. Como se ven los datos?"
      ],
      "metadata": {
        "id": "oyec1o6Q5isO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ayuda: fijate los parametros de spark.read.csv"
      ],
      "metadata": {
        "id": "GcJPxveiCkB7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usa la funcion show"
      ],
      "metadata": {
        "id": "Jln7qF2ReiPo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Como se ve el esquema?"
      ],
      "metadata": {
        "id": "qu3m6lVp5tIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#busca funciones para imprimir esquemas"
      ],
      "metadata": {
        "id": "TKj6FwxQ5vMy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecciona las primeras 3 columnas"
      ],
      "metadata": {
        "id": "IznCprUl6Hyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# usa el atributo .columns y slicing de listas"
      ],
      "metadata": {
        "id": "_dmGsTSdgCF_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#muestralas con un select"
      ],
      "metadata": {
        "id": "wlBmLuvtfpWE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Muestra un resumen estadistico de los datos numericos que podrian ser utiles"
      ],
      "metadata": {
        "id": "vO6voVp57ERT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mira la función summary"
      ],
      "metadata": {
        "id": "QIJ_CqBLg4vL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mira la función describe"
      ],
      "metadata": {
        "id": "dxQ05fPn6d5y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatea los numeros del anterior resultado usando las funciones de spark.sql que hay en F.\n",
        "\n",
        "nota: puedes usar ```dir(F)``` para tener una idea de que podria estar alli"
      ],
      "metadata": {
        "id": "_nyE0zgF45eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "ZSddct4LhUI8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# de pyspark.sql.functions hay una function llamada format_number trata de usarla"
      ],
      "metadata": {
        "id": "9cQVlGT58AbP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import format_number, col"
      ],
      "metadata": {
        "id": "0JidI_vkhzHb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resolucion:"
      ],
      "metadata": {
        "id": "KpVj99fhhzM6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# realiza una funcion que reciba un dataframe y retorne un dataframe donde la misma realice conteo de missing values para cada columna del dataframe de entrada (nulos y nan)"
      ],
      "metadata": {
        "id": "HWnCUswk8an2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# podrias necesitar las siguientes funciones count, isnan, isnull, when, col y usar comprehension de listas dentro del .select()"
      ],
      "metadata": {
        "id": "JzEsxU_hCufs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, isnan, isnull, when, col"
      ],
      "metadata": {
        "id": "AYbiPSLnkCZO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ILhq8SoJlEtf",
        "outputId": "8bd219cb-f220-4c19-d2f1-a254d50ad4db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fd46f52c9b30>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'app_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_df.select([count(when(isnull(column), column)).alias(column) for column in app_df.columns]).show(10, False)"
      ],
      "metadata": {
        "id": "qsXlB1_7k218"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_df.select([count(when(isnan(column) | isnull(column), column)).alias(column) for column in app_df.columns])"
      ],
      "metadata": {
        "id": "K8f4jLOpkChI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# null -> None\n",
        "# nan -> float"
      ],
      "metadata": {
        "id": "8_77w7qykCj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nan_and_nulls(spark_df):\n",
        "    return"
      ],
      "metadata": {
        "id": "pRI-Aj3gmqyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_nan_and_nulls(app_df)"
      ],
      "metadata": {
        "id": "mH41qPcwmq12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bDVu2Abmq5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# consigue un dataframe con solo las columnas de tipo numerico"
      ],
      "metadata": {
        "id": "fVouIjQpOPB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# existe un atributo llamado dtypes, explora que arroja y como podrias usar comprehension de listas para filtrar los tipos de las columnas dentro del .select()"
      ],
      "metadata": {
        "id": "jbhC0kL9OaOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_df.dtypes"
      ],
      "metadata": {
        "id": "7UlI3yYbo1s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solucion\n",
        "#Pista: app_df.select(list comprehension).show()"
      ],
      "metadata": {
        "id": "9orhvnRPo1xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iulEXDuao11e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# consigue un dataframe con solo las columnas con texto"
      ],
      "metadata": {
        "id": "6HBcHnskOpxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solucion parecida a la anterior"
      ],
      "metadata": {
        "id": "oJ3lwOFkDF16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# realiza un conteo de los distintos tipos de ingresos que existen"
      ],
      "metadata": {
        "id": "JG3b75fBPAnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Investiga la funcion distinct() para obtener los distintos tipos de ingresos"
      ],
      "metadata": {
        "id": "F60iWk2irDX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usa groupby , count y sort\n"
      ],
      "metadata": {
        "id": "fcoYu5c9DG1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# que tipo de income, educacion y genero tiene la persona que gana mas y la persona que gana la  ganancia media (o la mas cercana)"
      ],
      "metadata": {
        "id": "x4QWb7yiQ6Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# de pyspark.sql.functions puedes usar la funcion max y/o, pero tambien podrias no importar nada y usar el metodo\n",
        "# .agg() no tengas miedo de probar varias formas que se te ocurran\n",
        "# Puede que necesite usar el metodo .first() junto con un indexing, asi -> .first()[0]"
      ],
      "metadata": {
        "id": "nX7eHTOmRvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, mean"
      ],
      "metadata": {
        "id": "b5BOFUFaroaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Op-FLdIshTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaU1cQ5BroeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mWp0W86roht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SqxU0u8crorv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## como es la media de ingresos por genero"
      ],
      "metadata": {
        "id": "8ltMIknmXtkp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2cECjlJExEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# outliers\n",
        "\n",
        "una estrategia comun para conseguir outliers es ubicar aquellos puntos de la data que caigan fuera del rango ```-3 > z_score > 3```, usando esta estrategia conseguir el porcentaje de outliers de los ingresos, usar unicamente las funciones de ```pyspark.sql.functions```\n",
        "\n",
        "Z = (valor - valor_medio) / desviacion_estandar"
      ],
      "metadata": {
        "id": "tqLzxg3gYad2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# de pyspark.sql.functions tienes las funciones stddev y mean, usa un .withColumn() y luego tal ve un filtrado"
      ],
      "metadata": {
        "id": "4ieSvvLbbIWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import stddev, mean, col"
      ],
      "metadata": {
        "id": "RavXAuCEuVZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRN-zf0DuVdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jtrwzYruZJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xI7yZZ60uZNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JhMD7MlBuZQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXbsiILOu_pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2FYa3Wvu_sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0GA8s1jPu_vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# realiza para cada tipo de educacion en cada genero un conteo y la media de ingresos y guarda el resultado en un archivo csv"
      ],
      "metadata": {
        "id": "Wqj1TRtddPl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sabias que dentro del metodo groupBy se puede incluir una lista con nombres?? y el metodo .write() con el argumento header=True seguro te ayuda...\n"
      ],
      "metadata": {
        "id": "gqLFs8yTFSPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# realizar un join de ambas tablas por ID y conseguir el un conteo de cada toy_status por genero"
      ],
      "metadata": {
        "id": "6W52mb13g1aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# la clase asincronica y el ejercicio anterior seguro te ayudan en esta labor!\n"
      ],
      "metadata": {
        "id": "7lenvx7alzW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}